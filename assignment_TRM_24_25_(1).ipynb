{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junya2025/text-retrieval-and-mining/blob/main/assignment_TRM_24_25_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Apf-eCqgtl"
      },
      "source": [
        "# Document Analysis and Summarization System\n",
        "\n",
        "##Deadline\n",
        "**Friday January 24 by 23:59 at the latest**. Please do not submit your assignment after the deadline as late submissions will not be graded.\n",
        "\n",
        "## Learning Objectives:\n",
        "\n",
        "* Work with text data in Python\n",
        "* Understand basic text preprocessing\n",
        "* Use simple APIs for text analysis\n",
        "* Collaborate on a coding project\n",
        "* Create a basic command-line interface\n",
        "\n",
        "## Project Description:\n",
        "Your team of 4 will build a Python program that helps analyze and summarize documents. The program should:\n",
        "\n",
        "** Session 1 (~ 3 hours):\n",
        "\n",
        "\n",
        "* Read and preprocess text files\n",
        "* Calculate basic text statistics (word count, sentence count, average word length)\n",
        "* Find most common words and phrases\n",
        "* Generate and show 3 word clouds\n",
        "\n",
        "\n",
        "** Session 2 (~ 3 hours):\n",
        "\n",
        "\n",
        "* Use the Hugging Face Transformers library (https://huggingface.co/docs/hub/en/transformers) to: Generate summaries of the news articles.\n",
        "\n",
        "* Create a simple command-line interface to run all analyses\n",
        "* Save dataframe into a CSV file\n",
        "\n",
        "**Please note that I suggest the time that the assignment might take you. This is a mere guide and does not mean that is all the time you have. Take the time that you need**\n",
        "\n",
        "## Use the following News Articles Dataset:\n",
        "\n",
        "BBC News Dataset: https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive\n",
        "Contains ~2000 news articles in 5 categories. For your task, please use the column '**Content**' of this dataset. Use a sample of 500 news articles. Make sure your sample contains articles from all 5 categories.\n",
        "\n",
        "## Deliverable\n",
        "One self contained fully functional Notebook. Please send only the Notebook as your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBJeCxIaq7TK",
        "outputId": "18342ac6-9ff6-4f78-b081-34686c2a0d56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\junya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\junya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\junya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Import required libraries\n",
        "# Hint: You'll need nltk, pandas, matplotlib, wordcloud, and transformers\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Add more imports here...\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_EX5ZFBtpPS",
        "outputId": "c9284a8e-5f00-414d-f783-5727d3abc267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in c:\\users\\junya\\anaconda3\\lib\\site-packages (1.9.4)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in c:\\users\\junya\\anaconda3\\lib\\site-packages (from wordcloud) (10.3.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\junya\\anaconda3\\lib\\site-packages (from wordcloud) (3.8.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjZCG_R6tpPT",
        "outputId": "cdc63ab5-8fb6-4144-fb1e-8a433ff56ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
            "     ------------------ --------------------- 20.5/44.1 kB ? eta -:--:--\n",
            "     ------------------ --------------------- 20.5/44.1 kB ? eta -:--:--\n",
            "     ----------------------------------- -- 41.0/44.1 kB 245.8 kB/s eta 0:00:01\n",
            "     -------------------------------------- 44.1/44.1 kB 240.0 kB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Downloading safetensors-0.5.1-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\junya\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\junya\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.2/10.1 MB 7.3 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.5/10.1 MB 6.7 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 0.8/10.1 MB 6.6 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 1.1/10.1 MB 6.6 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.5/10.1 MB 6.6 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 1.8/10.1 MB 6.6 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 2.1/10.1 MB 6.3 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.3/10.1 MB 6.5 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 2.7/10.1 MB 6.3 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 2.9/10.1 MB 6.4 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 3.2/10.1 MB 6.3 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 3.4/10.1 MB 6.2 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 3.7/10.1 MB 6.3 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 4.1/10.1 MB 6.3 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 4.3/10.1 MB 6.1 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 4.5/10.1 MB 6.0 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 4.9/10.1 MB 6.3 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 5.2/10.1 MB 6.3 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 5.5/10.1 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 5.8/10.1 MB 6.3 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 6.2/10.1 MB 6.3 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 6.6/10.1 MB 6.4 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 7.0/10.1 MB 6.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 7.4/10.1 MB 6.6 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.7/10.1 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 8.0/10.1 MB 6.6 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 8.4/10.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 8.7/10.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.1/10.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 9.5/10.1 MB 6.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 9.8/10.1 MB 6.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.1/10.1 MB 6.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.1/10.1 MB 6.6 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
            "   ---------------------------------------- 0.0/450.7 kB ? eta -:--:--\n",
            "   ------------------------------ --------- 348.2/450.7 kB 7.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 450.7/450.7 kB 7.1 MB/s eta 0:00:00\n",
            "Downloading safetensors-0.5.1-cp38-abi3-win_amd64.whl (303 kB)\n",
            "   ---------------------------------------- 0.0/303.7 kB ? eta -:--:--\n",
            "   ---------------------------------------  297.0/303.7 kB 9.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 303.7/303.7 kB 9.2 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.4 MB 8.9 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 0.3/2.4 MB 8.9 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.5/2.4 MB 3.7 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.4/2.4 MB 7.6 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 1.8/2.4 MB 7.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.1/2.4 MB 7.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.4/2.4 MB 7.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 6.9 MB/s eta 0:00:00\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.27.1 safetensors-0.5.1 tokenizers-0.21.0 transformers-4.47.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnIsOaVvtpPT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2j7aEyotpPT",
        "outputId": "3981e7ca-8b38-4d40-89e8-198089033bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   category filename                              title  \\\n",
            "0  business  001.txt  Ad sales boost Time Warner profit   \n",
            "1  business  002.txt   Dollar gains on Greenspan speech   \n",
            "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
            "3  business  004.txt  High fuel prices hit BA's profits   \n",
            "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
            "\n",
            "                                             content  \n",
            "0   Quarterly profits at US media giant TimeWarne...  \n",
            "1   The dollar has hit its highest level against ...  \n",
            "2   The owners of embattled Russian oil giant Yuk...  \n",
            "3   British Airways has blamed high fuel prices f...  \n",
            "4   Shares in UK drinks and food firm Allied Dome...  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('bbc-news-data.csv', delimiter='\\t')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsPU8N11yOyO",
        "outputId": "940d75eb-7a58-4871-8369-d79d568c0b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    category filename                              title  \\\n",
            "0   business  001.txt  Ad sales boost Time Warner profit   \n",
            "1   business  002.txt   Dollar gains on Greenspan speech   \n",
            "2   business  003.txt  Yukos unit buyer faces loan claim   \n",
            "3   business  004.txt  High fuel prices hit BA's profits   \n",
            "4   business  005.txt  Pernod takeover talk lifts Domecq   \n",
            "5   business  006.txt   Japan narrowly escapes recession   \n",
            "6   business  007.txt   Jobs growth still slow in the US   \n",
            "7   business  008.txt   India calls for fair trade rules   \n",
            "8   business  009.txt  Ethiopia's crop production up 24%   \n",
            "9   business  010.txt  Court rejects $280bn tobacco case   \n",
            "10  business  011.txt  Ask Jeeves tips online ad revival   \n",
            "11  business  012.txt   Indonesians face fuel price rise   \n",
            "12  business  013.txt     Peugeot deal boosts Mitsubishi   \n",
            "13  business  014.txt   Telegraph newspapers axe 90 jobs   \n",
            "14  business  015.txt   Air passengers win new EU rights   \n",
            "\n",
            "                                              content  num_sentences  \\\n",
            "0    Quarterly profits at US media giant TimeWarne...             20   \n",
            "1    The dollar has hit its highest level against ...             15   \n",
            "2    The owners of embattled Russian oil giant Yuk...             12   \n",
            "3    British Airways has blamed high fuel prices f...             19   \n",
            "4    Shares in UK drinks and food firm Allied Dome...             12   \n",
            "5    Japan's economy teetered on the brink of a te...              9   \n",
            "6    The US created fewer jobs than expected in Ja...             14   \n",
            "7    India, which attends the G7 meeting of seven ...             15   \n",
            "8    Ethiopia produced 14.27 million tonnes of cro...             12   \n",
            "9    A US government claim accusing the country's ...             10   \n",
            "10   Ask Jeeves has become the third leading onlin...             11   \n",
            "11   Indonesia's government has confirmed it is co...             14   \n",
            "12   Struggling Japanese car maker Mitsubishi Moto...             14   \n",
            "13   The Daily and Sunday Telegraph newspapers are...             22   \n",
            "14   Air passengers who are unable to board their ...             28   \n",
            "\n",
            "    num_words  avg_word_length  avg_sentence_length  \\\n",
            "0         484         6.456621            20.750000   \n",
            "1         428         6.000000            25.266667   \n",
            "2         288         6.055172            21.500000   \n",
            "3         465         6.121622            21.052632   \n",
            "4         301         5.919463            21.666667   \n",
            "5         209         7.038462            20.000000   \n",
            "6         315         6.212500            19.785714   \n",
            "7         366         6.446328            21.000000   \n",
            "8         260         7.007463            18.666667   \n",
            "9         248         6.894309            22.200000   \n",
            "10        210         6.742268            16.272727   \n",
            "11        345         6.457143            21.071429   \n",
            "12        332         6.357576            21.071429   \n",
            "13        511         6.940959            20.954545   \n",
            "14        694         6.741742            21.428571   \n",
            "\n",
            "                                         top_10_words  \\\n",
            "0   [timewarner, aol, profits, said, internet, pro...   \n",
            "1   [us, deficit, dollar, months, recent, federal,...   \n",
            "2   [rosneft, yugansk, yukos, sale, menatep, asset...   \n",
            "3   [ba, fuel, said, results, year, revenue, costs...   \n",
            "4   [pernod, allied, domecq, shares, firm, seagram...   \n",
            "5   [economy, growth, japan, technical, recession,...   \n",
            "6   [jobs, said, us, january, rate, new, job, pres...   \n",
            "7   [india, nations, meeting, exchange, friday, fi...   \n",
            "8   [food, tonnes, million, agriculture, assistanc...   \n",
            "9   [tobacco, government, companies, smoking, cour...   \n",
            "10  [ask, jeeves, online, advertising, quarter, le...   \n",
            "11  [prices, government, fuel, subsidies, indonesi...   \n",
            "12  [mitsubishi, motors, peugeot, sales, japanese,...   \n",
            "13  [telegraph, said, journalists, newspapers, pri...   \n",
            "14  [airlines, passengers, compensation, new, flig...   \n",
            "\n",
            "                                        top_5_phrases  \n",
            "0   [fourth quarter, time warner, quarter profits,...  \n",
            "1   [federal reserve, current account, new york, r...  \n",
            "2   [menatep group, assets rosneft, owners embattl...  \n",
            "3   [fuel costs, aviation analyst, previously fore...  \n",
            "4   [allied domecq, shares uk, uk drinks, drinks f...  \n",
            "5   [japan economy, previously thought, economy te...  \n",
            "6   [unemployment rate, rate lowest, lowest level,...  \n",
            "7   [finance minister, india finance, mr chidambar...  \n",
            "8   [food assistance, million tonnes, food agricul...  \n",
            "9   [tobacco companies, us government, government ...  \n",
            "10  [ask jeeves, online advertising, jeeves become...  \n",
            "11  [fuel prices, yudhoyono government, fuel subsi...  \n",
            "12  [mitsubishi motors, car maker, last month, jap...  \n",
            "13  [new printing, sunday telegraph, telegraph gro...  \n",
            "14  [offer compensation, bad weather, air passenge...  \n"
          ]
        }
      ],
      "source": [
        "class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the DocumentAnalyzer with necessary resources\"\"\"\n",
        "        # TODO: Initialize stop words and the summarization pipeline\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.summarizer = None  # Initialize the summarization pipeline, leave none for now\n",
        "\n",
        "    def basic_stats(self, text):   #write this function first\n",
        "        \"\"\"Calculate basic text statistics\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the analysis results.\n",
        "        \"\"\"\n",
        "        # TODO: Calculate and return a dictionary containing:\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "        filtered_words = [word.lower() for word in words\n",
        "                         if word.isalpha() and word.lower() not in self.stop_words]\n",
        "        # - Number of sentences (num_sentences)\n",
        "        num_sentences = len(sentences)\n",
        "\n",
        "        # - Number of words (num_words)\n",
        "        num_words = len(words)\n",
        "\n",
        "        # - Average word length (avg_word_length)\n",
        "        avg_word_length = sum(len(word) for word in filtered_words) / len(filtered_words) if filtered_words else 0\n",
        "\n",
        "        # - Average sentence length (avg_sentence_length)\n",
        "        avg_sentence_length = sum(len(sent.split()) for sent in sentences) / num_sentences\n",
        "\n",
        "        return {\n",
        "            \"num_sentences\": num_sentences,\n",
        "            \"num_words\": num_words,\n",
        "            \"avg_word_length\": avg_word_length,\n",
        "            \"avg_sentence_length\": avg_sentence_length,\n",
        "            \"filtered_words\": filtered_words\n",
        "        }\n",
        "        pass\n",
        "\n",
        "    def process_dataframe(self, df, text_column):\n",
        "        \"\"\"Process text data from a pandas DataFrame column\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame\n",
        "            text_column (str): Name of the column containing text data\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with added analysis columns\n",
        "        \"\"\"\n",
        "        # TODO: Implement DataFrame processing\n",
        "        # 1. Read the 500 BBC news articles into the DataFrame. According to your selection criteria. separate by category!!!\n",
        "        random_sample_df = df.sample(n=500, random_state=42)\n",
        "\n",
        "        # 2. Apply text analysis functions to the 'content' column\n",
        "        stats_df = df[text_column].apply(self.basic_stats).apply(pd.Series)\n",
        "\n",
        "        # 3. Add and populate a new column for each of the following results: number of sentences (num_sentences), number of words (num_words),\n",
        "        #    average word length (avg_word_length), average sentence length (avg_sentence_length), top 10 most common words (common_words),\n",
        "        #    top 5 most common phrases (of lenght 2, i.e., two words),\n",
        "        #    Note: you have to make these calculations for each of the 500 news articles. Only use the 'content' column.\n",
        "        def get_top_10_words(words):\n",
        "            word_counts = Counter(words)\n",
        "            return [word for word, _ in word_counts.most_common(10)]\n",
        "        df['top_10_words'] = stats_df['filtered_words'].apply(get_top_10_words)\n",
        "\n",
        "        def get_top_phrases(words):\n",
        "            phrases = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
        "            phrase_counts = Counter(phrases)\n",
        "            return [phrase for phrase, _ in phrase_counts.most_common(5)]\n",
        "        df['top_5_phrases'] = stats_df['filtered_words'].apply(get_top_phrases)\n",
        "\n",
        "        # 4. Handle errors appropriately\n",
        "        try:\n",
        "            df['num_sentences'] = stats_df['num_sentences']\n",
        "            df['num_words'] = stats_df['num_words']\n",
        "            df['avg_word_length'] = stats_df['avg_word_length']\n",
        "            df['avg_sentence_length'] = stats_df['avg_sentence_length']\n",
        "            df['top_10_words'] = df['top_10_words']\n",
        "            df['top_5_phrases'] = df['top_5_phrases']\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing DataFrame: {e}\")\n",
        "            return df\n",
        "\n",
        "        pass\n",
        "\n",
        "    def get_common_words(self, text, n=10):\n",
        "        \"\"\"Find the n most common words in the text\"\"\"\n",
        "        # TODO: Implement word frequency analysis\n",
        "        # Remember to:\n",
        "        # 1. Tokenize the text\n",
        "        word = word_tokenize(text)\n",
        "        # 2. Convert to lowercase\n",
        "        word = [word.lower() for word in words]\n",
        "        # 3. Remove stopwords\n",
        "        words = [word for word in words if word.isalpha() and word not in self.stop_words]\n",
        "        # 4. Count frequencies\n",
        "        word_counts = Counter(words)\n",
        "        return word_counts.most_common(n)\n",
        "        pass\n",
        "\n",
        "    def get_common_phrases(self, text, n=5, phrase_length=2):\n",
        "        \"\"\"Find the n most common phrases of specified length\"\"\"\n",
        "        # TODO: Implement phrase frequency analysis using nltk.util.ngrams\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word not in self.stop_words]\n",
        "        phrases = list(ngrams(words, phrase_length))\n",
        "        phrase_counts = Counter(phrases)\n",
        "        return phrase_counts.most_common(n)\n",
        "\n",
        "        pass\n",
        "\n",
        "    def create_wordcloud(self, text)\n",
        "\n",
        "analyzer = DocumentAnalyzer()\n",
        "processed_df = analyzer.process_dataframe(df, 'content')\n",
        "\n",
        "print(processed_df.head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvDEqw3JtpPU"
      },
      "outputs": [],
      "source": [
        "class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the DocumentAnalyzer with necessary resources\"\"\"\n",
        "        # TODO: Initialize stop words and the summarization pipeline\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "\n",
        "    def basic_stats(self, text):\n",
        "        \"\"\"Calculate basic text statistics\"\"\"\n",
        "        # TODO: Calculate and return a dictionary containing:\n",
        "        # - Number of sentences (num_sentences)\n",
        "        # - Number of words (num_words)\n",
        "        # - Average word length (avg_word_length)\n",
        "        # - Average sentence length (avg_sentence_length)\n",
        "        pass\n",
        "\n",
        "    def process_dataframe(self, df, text_column):\n",
        "        \"\"\"Process text data from a pandas DataFrame column\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame\n",
        "            text_column (str): Name of the column containing text data\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with added analysis columns\n",
        "        \"\"\"\n",
        "        # TODO: Implement DataFrame processing\n",
        "        # 1. Read the 500 BBC news articles into the DataFrame. According to your selection criteria.\n",
        "        random_sample_df = df.sample(n=500, random_state=42)\n",
        "\n",
        "        # 2. Apply text analysis functions to the 'content' column\n",
        "        analyzer = DocumentAnalyzer()\n",
        "        stats = df['content'].apply(analyzer.basic_stats).apply(pd.Series)\n",
        "\n",
        "        # 3. Add and populate a new column for each of the following results: number of sentences (num_sentences), number of words (num_words),\n",
        "        #    average word length (avg_word_length), average sentence length (avg_sentence_length), top 10 most common words (common_words),\n",
        "        #    top 5 most common phrases (of lenght 2, i.e., two words),\n",
        "        #    Note: you have to make these calculations for each of the 500 news articles. Only use the 'content' column.\n",
        "        word_counts = Counter(filtered_words)\n",
        "        top_10_words = [word for word, _ in word_counts.most_common(10)]\n",
        "\n",
        "        phrases = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
        "        phrase_counts = Counter(phrases)\n",
        "        top_5_phrases = [phrase for phrase, _ in phrase_counts.most_common(5)]\n",
        "\n",
        "\n",
        "        # 4. Handle errors appropriately\n",
        "        try:\n",
        "            df['num_sentences'] = df[text_column].apply(self.analyze_text).apply(lambda x: x['num_sentences'])\n",
        "            df['num_words'] = df[text_column].apply(self.analyze_text).apply(lambda x: x['num_words'])\n",
        "            df['avg_word_length'] = df[text_column].apply(self.analyze_text).apply(lambda x: x['avg_word_length'])\n",
        "            df['avg_sentence_length'] = df[text_column].apply(self.analyze_text).apply(lambda x: x['avg_sentence_length'])\n",
        "            df['common_words'] = df[text_column].apply(self.analyze_text).apply(lambda x: x['common_words'])\n",
        "            df['common_phrases'] = df[text_column].apply(self.analyze_text).apply(lambda x: x['common_phrases'])\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing DataFrame: {e}\")\n",
        "            return df\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "    def get_common_words(self, text, n=10):\n",
        "        \"\"\"Find the n most common words in the text\"\"\"\n",
        "        # TODO: Implement word frequency analysis\n",
        "        # Remember to:\n",
        "        # 1. Tokenize the text\n",
        "        word = word_tokenize(text)\n",
        "        # 2. Convert to lowercase\n",
        "        word = [word.lower() for word in words]\n",
        "        # 3. Remove stopwords\n",
        "        words = [word for word in words if word.isalpha() and word not in self.stop_words]\n",
        "        # 4. Count frequencies\n",
        "        word_counts = Counter(words)\n",
        "        return word_counts.most_common(n)\n",
        "\n",
        "        pass\n",
        "\n",
        "    def get_common_phrases(self, text, n=5, phrase_length=2):\n",
        "        \"\"\"Find the n most common phrases of specified length\"\"\"\n",
        "        # TODO: Implement phrase frequency analysis using nltk.util.ngrams\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word not in self.stop_words]\n",
        "        phrases = list(ngrams(words, phrase_length))\n",
        "        phrase_counts = Counter(phrases)\n",
        "        return phrase_counts.most_common(n)\n",
        "\n",
        "        pass\n",
        "\n",
        "    def create_wordcloud(self, text):\n",
        "        \"\"\"Generate and save a word cloud visualization\"\"\"\n",
        "        # TODO: Create and show word cloud visualizations of the most common words for 3 randomly selected news articles.\n",
        "        # Use WordCloud class and matplotlib\n",
        "        for article in random_sample_df['content']:\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(article)\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        pass\n",
        "\n",
        "    def generate_summary(self, text, max_length=50, min_length=30):\n",
        "        \"\"\"Generate a summary using the BART model\"\"\"\n",
        "        # TODO: Implement text summarization\n",
        "        # Remember to:\n",
        "        # 1. Handle long texts by splitting into chunks\n",
        "        chunk_size = 512\n",
        "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "        # 2. Use the summarization pipeline\n",
        "        summaries = []\n",
        "        for chunk in chunks:\n",
        "            summary = self.summarizer(chunk, max_length=max_length, min_length=min_length)[0]['summary_text']\n",
        "            summaries.append(summary)\n",
        "        # 3. Combine summaries if needed\n",
        "        combined_summary = \" \".join(summaries)\n",
        "        return combined_summary\n",
        "\n",
        "        pass\n",
        "\n",
        "    def _split_into_chunks(self, text, max_chunk_size=1000):\n",
        "        \"\"\"Helper method to split text into chunks for summarization\"\"\"\n",
        "        # TODO: Implement text splitting into chunks\n",
        "        # This is needed because the summarizer has a maximum input length\n",
        "        all_tokens = self.tokenizer.encode(text)\n",
        "        chunks = []\n",
        "        for i in range(0, len(all_tokens), max_chunk_size):\n",
        "            chunk_tokens = all_tokens[i:i + max_chunk_size]\n",
        "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
        "            chunks.append(chunk_text)\n",
        "        return chunks\n",
        "\n",
        "        pass\n",
        "\n",
        "    def save_analysis(self, filepath, analysis_results):\n",
        "        \"\"\"Save analysis results to a CSV file\"\"\"\n",
        "        # TODO: Implement saving dataframe to a CSV file\n",
        "        analysis_results.to_csv(filepath, index=False)\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej_Da38WtpPU"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the DocumentAnalyzer with necessary resources\"\"\"\n",
        "        # TODO: Initialize stop words and the summarization pipeline\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.summarizer = None  # Initialize the summarization pipeline, leave blank for now\n",
        "\n",
        "    def process_dataframe(self, df, text_column):\n",
        "        \"\"\"Process text data from a pandas DataFrame column\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame\n",
        "            text_column (str): Name of the column containing text data\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with added analysis columns\n",
        "        \"\"\"\n",
        "        # TODO: Implement DataFrame processing\n",
        "        # 1. Read the 500 BBC news articles into the DataFrame. According to your selection criteria.\n",
        "        random_sample_df = df.sample(n=500, random_state=42)\n",
        "\n",
        "        # 2. Apply text analysis functions to the 'content' column\n",
        "        analyzer = DocumentAnalyzer()\n",
        "        stats = df['content'].apply(analyzer.basic_stats).apply(pd.Series)\n",
        "\n",
        "        # 3. Add and populate a new column for each of the following results: number of sentences (num_sentences), number of words (num_words),\n",
        "        #    average word length (avg_word_length), average sentence length (avg_sentence_length), top 10 most common words (common_words),\n",
        "        #    top 5 most common phrases (of lenght 2, i.e., two words),\n",
        "        #    Note: you have to make these calculations for each of the 500 news articles. Only use the 'content' column.\n",
        "        # 4. Handle errors appropriately\n",
        "        pass\n",
        "\n",
        "    def basic_stats(self, text):\n",
        "        \"\"\"Calculate basic text statistics\"\"\"\n",
        "        # TODO: Calculate and return a dictionary containing:\n",
        "        # - Number of sentences (num_sentences)\n",
        "        # - Number of words (num_words)\n",
        "        # - Average word length (avg_word_length)\n",
        "        # - Average sentence length (avg_sentence_length)\n",
        "        pass\n",
        "\n",
        "    def get_common_words(self, text, n=10):\n",
        "        \"\"\"Find the n most common words in the text\"\"\"\n",
        "        # TODO: Implement word frequency analysis\n",
        "        # Remember to:\n",
        "        # 1. Tokenize the text\n",
        "        # 2. Convert to lowercase\n",
        "        # 3. Remove stopwords\n",
        "        # 4. Count frequencies\n",
        "        pass\n",
        "\n",
        "    def get_common_phrases(self, text, n=5, phrase_length=2):\n",
        "        \"\"\"Find the n most common phrases of specified length\"\"\"\n",
        "        # TODO: Implement phrase frequency analysis using nltk.util.ngrams\n",
        "        pass\n",
        "\n",
        "    def create_wordcloud(self, text):\n",
        "        \"\"\"Generate and save a word cloud visualization\"\"\"\n",
        "        # TODO: Create and show word cloud visualizations of the most common words for 3 randomly selected news articles.\n",
        "        # Use WordCloud class and matplotlib\n",
        "        pass\n",
        "\n",
        "    def generate_summary(self, text, max_length=50, min_length=30):\n",
        "        \"\"\"Generate a summary using the BART model\"\"\"\n",
        "        # TODO: Implement text summarization\n",
        "        # Remember to:\n",
        "        # 1. Handle long texts by splitting into chunks\n",
        "        # 2. Use the summarization pipeline\n",
        "        # 3. Combine summaries if needed\n",
        "        pass\n",
        "\n",
        "    def _split_into_chunks(self, text, max_chunk_size=1000):\n",
        "        \"\"\"Helper method to split text into chunks for summarization\"\"\"\n",
        "        # TODO: Implement text splitting into chunks\n",
        "        # This is needed because the summarizer has a maximum input length\n",
        "        pass\n",
        "\n",
        "    def save_analysis(self, filepath, analysis_results):\n",
        "        \"\"\"Save analysis results to a CSV file\"\"\"\n",
        "        # TODO: Implement saving dataframe to a CSV file\n",
        "        pass\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDzbpPzeyYct"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # TODO: Implement the main function that:\n",
        "    # 1. Initializes the analyzer\n",
        "    # 2. Populates the DataFrame with 500 BBC news articles\n",
        "    # 3. Processes the DataFrame\n",
        "    # 4. Generates worldclouds for 5 random summaries\n",
        "    # 5. Saves the resulting updated dataframe to CSV\n",
        "\n",
        "    # Hint: Start by loading your 500 news articles into a dataframe. Make sure to include the code to select those 500 news articles.\n",
        "    filepath = #path_to_data_file - Replace with your text file\n",
        "    df = pd.DataFrame(pd.read_csv(filepath, sep='\t', engine='python'))\n",
        "    # TODO: Complete the implementation\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Testing functions\n",
        "def run_tests():\n",
        "    # TODO: Implement tests for your functions\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}