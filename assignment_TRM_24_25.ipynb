{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Apf-eCqgtl"
      },
      "source": [
        "# Document Analysis and Summarization System\n",
        "\n",
        "##Deadline\n",
        "**Friday January 24 by 23:59 at the latest**. Please do not submit your assignment after the deadline as late submissions will not be graded.\n",
        "\n",
        "## Learning Objectives:\n",
        "\n",
        "* Work with text data in Python\n",
        "* Understand basic text preprocessing\n",
        "* Use simple APIs for text analysis\n",
        "* Collaborate on a coding project\n",
        "* Create a basic command-line interface\n",
        "\n",
        "## Project Description:\n",
        "Your team of 4 will build a Python program that helps analyze and summarize documents. The program should:\n",
        "\n",
        "** Session 1 (~ 3 hours):\n",
        "\n",
        "\n",
        "* Read and preprocess text files\n",
        "* Calculate basic text statistics (word count, sentence count, average word length)\n",
        "* Find most common words and phrases\n",
        "* Generate and show 3 word clouds\n",
        "\n",
        "\n",
        "** Session 2 (~ 3 hours):\n",
        "\n",
        "\n",
        "* Use the Hugging Face Transformers library (https://huggingface.co/docs/hub/en/transformers) to: Generate summaries of the news articles.\n",
        "\n",
        "* Create a simple command-line interface to run all analyses\n",
        "* Save dataframe into a CSV file\n",
        "\n",
        "**Please note that I suggest the time that the assignment might take you. This is a mere guide and does not mean that is all the time you have. Take the time that you need**\n",
        "\n",
        "## Use the following News Articles Dataset:\n",
        "\n",
        "BBC News Dataset: https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive\n",
        "Contains ~2000 news articles in 5 categories. For your task, please use the column '**Content**' of this dataset. Use a sample of 500 news articles. Make sure your sample contains articles from all 5 categories.\n",
        "\n",
        "## Deliverable\n",
        "One self contained fully functional Notebook. Please send only the Notebook as your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBJeCxIaq7TK"
      },
      "outputs": [],
      "source": [
        "# TODO: Import required libraries\n",
        "# Hint: You'll need nltk, pandas, matplotlib, wordcloud, and transformers\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# Add more imports here...\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('bbc-news-data.csv', delimiter='\\t')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsPU8N11yOyO"
      },
      "outputs": [],
      "source": [
        "class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the DocumentAnalyzer with necessary resources\"\"\"\n",
        "        # TODO: Initialize stop words and the summarization pipeline\n",
        "        self.stop_words = set(stopwords.words('english')) \n",
        "        self.summarizer = None  # Initialize the summarization pipeline, leave none for now\n",
        "\n",
        "        def basic_stats(self, text):   #write this function first\n",
        "        \"\"\"Calculate basic text statistics\n",
        "        \n",
        "        Args:\n",
        "            text (str): The input text.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the analysis results.\n",
        "        \"\"\"\n",
        "        # TODO: Calculate and return a dictionary containing:\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "        filtered_words = [word.lower() for word in words \n",
        "                         if word.isalpha() and word.lower() not in self.stop_words]\n",
        "        # - Number of sentences (num_sentences)\n",
        "        num_sentences = len(sentences)\n",
        "\n",
        "        # - Number of words (num_words)\n",
        "        num_words = len(words)\n",
        "        \n",
        "        # - Average word length (avg_word_length)\n",
        "        avg_word_length = sum(len(word) for word in filtered_words) / len(filtered_words) if filtered_words else 0\n",
        "        \n",
        "        # - Average sentence length (avg_sentence_length) \n",
        "        avg_sentence_length = sum(len(sent.split()) for sent in sentences) / num_sentences \n",
        "        \n",
        "        return {\n",
        "            \"num_sentences\": num_sentences,\n",
        "            \"num_words\": num_words,\n",
        "            \"avg_word_length\": avg_word_length,\n",
        "            \"avg_sentence_length\": avg_sentence_length,\n",
        "            \"filtered_words\": filtered_words\n",
        "        }\n",
        "        pass\n",
        "\n",
        "\n",
        "    def process_dataframe(self, df, text_column):\n",
        "        \"\"\"Process text data from a pandas DataFrame column\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame\n",
        "            text_column (str): Name of the column containing text data\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with added analysis columns\n",
        "        \"\"\"\n",
        "        # TODO: Implement DataFrame processing\n",
        "        # 1. Read the 500 BBC news articles into the DataFrame. According to your selection criteria. separate by category!!!\n",
        "        random_sample_df = df.sample(n=500, random_state=42) \n",
        "        \n",
        "        # 2. Apply text analysis functions to the 'content' column\n",
        "        stats_df = df[text_column].apply(self.basic_stats).apply(pd.Series)\n",
        "\n",
        "        # 3. Add and populate a new column for each of the following results: number of sentences (num_sentences), number of words (num_words),\n",
        "        #    average word length (avg_word_length), average sentence length (avg_sentence_length), top 10 most common words (common_words),\n",
        "        #    top 5 most common phrases (of lenght 2, i.e., two words),\n",
        "        #    Note: you have to make these calculations for each of the 500 news articles. Only use the 'content' column.\n",
        "        def get_top_10_words(words):\n",
        "            word_counts = Counter(words)\n",
        "            return [word for word, _ in word_counts.most_common(10)]\n",
        "        df['top_10_words'] = stats_df['filtered_words'].apply(get_top_10_words) \n",
        "\n",
        "        def get_top_phrases(words):\n",
        "            phrases = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
        "            phrase_counts = Counter(phrases)\n",
        "            return [phrase for phrase, _ in phrase_counts.most_common(5)]\n",
        "        df['top_5_phrases'] = stats_df['filtered_words'].apply(get_top_phrases) \n",
        "        \n",
        "        # 4. Handle errors appropriately\n",
        "        try:\n",
        "            df['num_sentences'] = stats_df['num_sentences']\n",
        "            df['num_words'] = stats_df['num_words']\n",
        "            df['avg_word_length'] = stats_df['avg_word_length']\n",
        "            df['avg_sentence_length'] = stats_df['avg_sentence_length']\n",
        "            df['top_10_words'] = df['top_10_words']\n",
        "            df['top_5_phrases'] = df['top_5_phrases']\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing DataFrame: {e}\")\n",
        "            return df\n",
        "\n",
        "        pass\n",
        "\n",
        "    def get_common_words(self, text, n=10):\n",
        "        \"\"\"Find the n most common words in the text\"\"\"\n",
        "        # TODO: Implement word frequency analysis\n",
        "        # Remember to:\n",
        "        # 1. Tokenize the text\n",
        "        word = word_tokenize(text)\n",
        "        # 2. Convert to lowercase\n",
        "        word = [word.lower() for word in words]\n",
        "        # 3. Remove stopwords\n",
        "        words = [word for word in words if word.isalpha() and word not in self.stop_words] \n",
        "        # 4. Count frequencies\n",
        "        word_counts = Counter(words)\n",
        "        return word_counts.most_common(n)\n",
        "        pass\n",
        "\n",
        "    def get_common_phrases(self, text, n=5, phrase_length=2):\n",
        "        \"\"\"Find the n most common phrases of specified length\"\"\"\n",
        "        # TODO: Implement phrase frequency analysis using nltk.util.ngrams\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word not in self.stop_words]\n",
        "        phrases = list(ngrams(words, phrase_length))\n",
        "        phrase_counts = Counter(phrases)\n",
        "        return phrase_counts.most_common(n)\n",
        "\n",
        "        pass\n",
        "\n",
        "    def create_wordcloud(self, text):\n",
        "        \"\"\"Generate and save a word cloud visualization\"\"\"\n",
        "        # TODO: Create and show word cloud visualizations of the most common words for 3 randomly selected news articles.\n",
        "        # Use WordCloud class and matplotlib\n",
        "        pass\n",
        "\n",
        "    def generate_summary(self, text, max_length=50, min_length=30):\n",
        "        \"\"\"Generate a summary using the BART model\"\"\"\n",
        "        # TODO: Implement text summarization\n",
        "        # Remember to:\n",
        "        # 1. Handle long texts by splitting into chunks\n",
        "        # 2. Use the summarization pipeline\n",
        "        # 3. Combine summaries if needed\n",
        "        pass\n",
        "\n",
        "    def _split_into_chunks(self, text, max_chunk_size=1000):\n",
        "        \"\"\"Helper method to split text into chunks for summarization\"\"\"\n",
        "        # TODO: Implement text splitting into chunks\n",
        "        # This is needed because the summarizer has a maximum input length\n",
        "        pass\n",
        "\n",
        "    def save_analysis(self, filepath, analysis_results):\n",
        "        \"\"\"Save analysis results to a CSV file\"\"\"\n",
        "        # TODO: Implement saving dataframe to a CSV file\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDzbpPzeyYct"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # TODO: Implement the main function that:\n",
        "    # 1. Initializes the analyzer\n",
        "    # 2. Populates the DataFrame with 500 BBC news articles\n",
        "    # 3. Processes the DataFrame\n",
        "    # 4. Generates worldclouds for 5 random summaries\n",
        "    # 5. Saves the resulting updated dataframe to CSV\n",
        "\n",
        "    # Hint: Start by loading your 500 news articles into a dataframe. Make sure to include the code to select those 500 news articles.\n",
        "    filepath = #path_to_data_file - Replace with your text file\n",
        "    df = pd.DataFrame(pd.read_csv(filepath, sep='\t', engine='python'))\n",
        "    # TODO: Complete the implementation\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Testing functions\n",
        "def run_tests():\n",
        "    # TODO: Implement tests for your functions\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
