{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junya2025/text-retrieval-and-mining/blob/main/tutorial_II_applications_of_text_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgCdx3uqdeSF"
      },
      "source": [
        "# Tutorial II: Applications of Text Analysis\n",
        "\n",
        "**Duration:** 1.5 hours\n",
        "\n",
        "**Prerequisites:** Basic Python knowledge, familiarity with pandas and numpy\n",
        "\n",
        "**Learning Objectives**\n",
        "* Implement document classification\n",
        "* Create document clustering solutions\n",
        "* Build sentiment analysis models\n",
        "* Perform Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QMY9e5bmda_5"
      },
      "outputs": [],
      "source": [
        "## Setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "from textblob import TextBlob\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BLEQaTseCv4"
      },
      "source": [
        "## Exercise 1: Document Classification (25 minutes)\n",
        "\n",
        "In this exercise, you'll implement a document classifier using TF-IDF and Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0EgKOYudlgD",
        "outputId": "ad48bb8d-810a-455c-ff1a-8e8ff01807ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted categories: ['finance' 'finance']\n"
          ]
        }
      ],
      "source": [
        "def create_document_classifier():\n",
        "    \"\"\"\n",
        "    Create a pipeline for document classification using TF-IDF and Naive Bayes\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: sklearn pipeline for document classification\n",
        "    \"\"\"\n",
        "    # TODO: Create a pipeline with:\n",
        "    # 1. TfidfVectorizer for converting text to numerical features\n",
        "    # 2. MultinomialNB for classification (If you do not know what MultinomialNB is, feel free to do some research before proceeding)\n",
        "    pass\n",
        "\n",
        "# Example documents and labels\n",
        "documents = [\n",
        "    \"The stock market saw significant gains today\",\n",
        "    \"Scientists discover new species in Amazon\",\n",
        "    \"Team wins championship game in overtime\",\n",
        "    \"New economic policy impacts global markets\"\n",
        "]\n",
        "labels = ['finance', 'science', 'sports', 'finance']\n",
        "\n",
        "# TODO: Create and train classifier\n",
        "classifier = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# training the model\n",
        "classifier.fit(documents, labels)\n",
        "\n",
        "# Test your classifier\n",
        "new_docs = [\n",
        "    \"Investors worried about market trends\",\n",
        "    \"Researchers study rare wildlife behavior\"\n",
        "]\n",
        "# TODO: Make predictions on new documents\n",
        "predictions = classifier.predict(new_docs)\n",
        "\n",
        "print(\"Predicted categories:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Zm2me9eJ8F"
      },
      "source": [
        "## Exercise 2: Document Clustering (25 minutes)\n",
        "\n",
        "Implement unsupervised document clustering using K-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3vRV_l4dzXi",
        "outputId": "7b09188f-d80c-4265-ffa5-e7f4cc89511c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0: data, is, knowledge, analysis, statistical, requires, programming, essential, python, science\n",
            "Cluster 1: are, neural, networks, for, biological, brains, by, inspired, and, statistics\n"
          ]
        }
      ],
      "source": [
        "def cluster_documents(documents, n_clusters=2):\n",
        "    \"\"\"\n",
        "    Cluster documents using TF-IDF and K-means\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of text documents\n",
        "        n_clusters (int): Number of clusters\n",
        "\n",
        "    Returns:\n",
        "        tuple: (cluster_assignments, top_terms_per_cluster)\n",
        "    \"\"\"\n",
        "    # TODO: Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # TODO: Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # TODO: Get top terms for each cluster\n",
        "    # Hint: Look at cluster centers and feature names\n",
        "\n",
        "    cluster_assignments = kmeans.labels_ # Get cluster assignments for each document\n",
        "    top_terms_per_cluster = {}\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    for i in range(n_clusters):\n",
        "        order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]  # Sort indices of features by descending order of weight\n",
        "        top_terms = [feature_names[ind] for ind in order_centroids[i, :5]]  # Get top 5 terms\n",
        "        top_terms_per_cluster[i] = top_terms\n",
        "\n",
        "    return (cluster_assignments, top_terms_per_cluster)\n",
        "\n",
        "# Test documents\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Deep learning uses neural networks for AI tasks\",\n",
        "    \"Python programming is essential for data science\",\n",
        "    \"Data analysis requires statistical knowledge\",\n",
        "    \"Neural networks are inspired by biological brains\",\n",
        "    \"Statistics and probability are foundational for ML\"\n",
        "]\n",
        "\n",
        "# TODO: Cluster documents and print results\n",
        "clusters, top_terms = cluster_documents(documents, n_clusters=2)\n",
        "for cluster, terms in top_terms.items():\n",
        "    print(f\"Cluster {cluster}: {', '.join(terms)}\")\n",
        "\n",
        "#stop words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To-glcdGeRcj"
      },
      "source": [
        "## Exercise 3: Sentiment Analysis (20 minutes)\n",
        "\n",
        "Implement both dictionary-based and ML-based sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMoIg951eSAg",
        "outputId": "e2f431d6-105c-49b4-e942-eaa7320559f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: This product exceeded my expectations!\n",
            "Sentiment: Neutral\n",
            "\n",
            "Text: Terrible customer service, would not recommend.\n",
            "Sentiment: Negative\n",
            "\n",
            "Text: The item was okay, nothing special.\n",
            "Sentiment: Positive\n",
            "\n",
            "Predictions: [ 1 -1  0]\n"
          ]
        }
      ],
      "source": [
        "def analyze_sentiment_dictionary(text):\n",
        "    \"\"\"\n",
        "    Analyze sentiment using TextBlob's pre-trained model\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        str: 'Positive', 'Negative', or 'Neutral'\n",
        "    \"\"\"\n",
        "    # TODO: Use TextBlob to analyze sentiment\n",
        "    # Hint: Look at the sentiment.polarity value\n",
        "    analysis = TextBlob(text)\n",
        "    polarity = analysis.sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 'Positive'\n",
        "    elif polarity < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "    pass\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def create_ml_sentiment_analyzer(texts, labels):\n",
        "    \"\"\"\n",
        "    Create and train ML-based sentiment analyzer\n",
        "\n",
        "    Args:\n",
        "        texts (list): Training texts\n",
        "        labels (list): Sentiment labels\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: Trained sentiment analyzer\n",
        "    \"\"\"\n",
        "    # TODO: Create and train a pipeline for sentiment analysis\n",
        "    # Hint: Similar to document classifier\n",
        "    sentiment_analyzer = Pipeline([\n",
        "      ('tfidf', TfidfVectorizer()),\n",
        "      ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    # Train the pipeline on the provided data\n",
        "    sentiment_analyzer.fit(texts, labels)\n",
        "\n",
        "    return sentiment_analyzer\n",
        "\n",
        "    pass\n",
        "\n",
        "# Test data\n",
        "reviews = [\n",
        "    \"This product exceeded my expectations!\",\n",
        "    \"Terrible customer service, would not recommend.\",\n",
        "    \"The item was okay, nothing special.\",\n",
        "]\n",
        "\n",
        "# TODO: Test dictionary-based approach\n",
        "for review in reviews:\n",
        "    sentiment = analyze_sentiment_dictionary(review)\n",
        "    print(f\"Text: {review}\")\n",
        "    print(f\"Sentiment: {sentiment}\\n\")\n",
        "\n",
        "# TODO: Test ML-based approach\n",
        "train_texts = [\n",
        "    \"great product\", \"terrible service\", \"okay experience\"\n",
        "]\n",
        "train_labels = [1, -1, 0]  # 1=positive, -1=negative, 0=neutral\n",
        "\n",
        "classifier = create_ml_sentiment_analyzer(train_texts, train_labels)\n",
        "\n",
        "predictions = classifier.predict(reviews)\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLygJP__ec3f"
      },
      "source": [
        "## Exercise 4: Named Entity Recognition (20 minutes)\n",
        "\n",
        "Implement Named Entity Recognition (NER) using spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF94KKgfedUt",
        "outputId": "e0682f04-1168-41f5-b90a-a261f5b3be27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Elon Musk', 'PERSON'), ('2002', 'DATE'), ('Tesla', 'ORG'), ('California', 'GPE')]\n"
          ]
        }
      ],
      "source": [
        "def perform_ner(text):\n",
        "    \"\"\"\n",
        "    Perform Named Entity Recognition using spaCy\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        list: List of (entity_text, entity_label) tuples\n",
        "    \"\"\"\n",
        "    # TODO: Load spaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # TODO: Process text and extract entities\n",
        "    # Hint: Look at doc.ents\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append((ent.text, ent.label_))\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Test text\n",
        "text = \"When Elon Musk founded SpaceX in 2002, Tesla was already operating in California.\"\n",
        "\n",
        "# TODO: Extract and print entities\n",
        "entities = perform_ner(text)\n",
        "print(entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgU3eVbudp3N"
      },
      "source": [
        "## Discussion Questions - Food for thought\n",
        "1. What are the advantages and limitations of TF-IDF for text classification?\n",
        "+ to convert text into strings so that machine can understand\n",
        "+ can handle high-dimentional data\n",
        "- sensitive to stop words\n",
        "- do not capture word order\n",
        "2. How do you determine the optimal number of clusters?\n",
        "Silhouette, elbow method\n",
        "3. What are the challenges in sentiment analysis?\n",
        "scarsism detection\n",
        "4. How can NER be improved for domain-specific applications?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
